{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1500, 5)\n",
    "e = torch.randn(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = e*0.1 + torch.sin(a[:, 0]*10 + 20*a[:, 1]**3) + \\\n",
    "    torch.cos((3*a[:, 2] + 5*a[:, 3]**2)) + \\\n",
    "    2/(torch.sqrt(1 + torch.relu(a[:, 4] + 0.05))) + \\\n",
    "    2 * a[:, 4] * a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, b):\n",
    "    return torch.square(y-b).mean()\n",
    "\n",
    "# def loss(y, b):\n",
    "#     use_loss = torch.square(y-b).mean()\n",
    "#     return (use_loss * torch.clamp(torch.exp(use_loss), max=10)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8433), tensor(2.1302))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean(), b.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = a[1000:]\n",
    "bb = b[1000:]\n",
    "a = a[:1000]\n",
    "b = b[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super(Multiply, self).__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x*self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 5]) torch.Size([500]) torch.Size([1000, 5]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print (aa.shape, bb.shape, a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "all_res = []\n",
    "all_losses_alpha_more2 = {}\n",
    "all_test_losses_alpha_more2 = {}\n",
    "m = 5000\n",
    "a = a.to(device)\n",
    "b = b.to(device)\n",
    "aa = aa.to(device)\n",
    "bb = bb.to(device)\n",
    "j = 0\n",
    "for j in range(10):\n",
    "    torch.manual_seed(j*77+2951)\n",
    "    for alpha in [-2.0, -1, 0.05, 0, 0.01]:\n",
    "        z2 = nn.Sequential(nn.Linear(5, m), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, 1))\n",
    "        nn.init.constant_(z2[0].bias, 0)\n",
    "        nn.init.normal_(z2[0].weight, mean=0, std=np.sqrt(1./m))\n",
    "        nn.init.constant_(z2[1].bias, 0)\n",
    "        nn.init.normal_(z2[1].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.normal_(z2[4].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[4].bias, 0)\n",
    "        nn.init.normal_(z2[7].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[7].bias, 0)\n",
    "        nn.init.normal_(z2[10].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[10].bias, 0)\n",
    "        nn.init.normal_(z2[13].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[13].bias, 0)\n",
    "        nn.init.normal_(z2[16].weight, mean=0, std=np.sqrt(1))\n",
    "        nn.init.constant_(z2[16].bias, 0)\n",
    "        z2 = z2.to(device)\n",
    "        all_losses2 = []\n",
    "        all_test_losses2 = []\n",
    "        op2 = optim.SGD(lr=1e-4, params=z2.parameters())\n",
    "        print (\"=============\", alpha, \"===============\")\n",
    "        for i in range(5000):\n",
    "            op2.zero_grad()\n",
    "            yh = z2(a).view(-1)\n",
    "            loss_use = loss(yh, b)\n",
    "            loss_use.backward()\n",
    "            op2.step()\n",
    "            all_losses2.append(loss_use.item())\n",
    "            yh = z2(aa).view(-1)\n",
    "            loss_use = loss(yh, bb)\n",
    "            all_test_losses2.append(loss_use.item())\n",
    "            if i % 5000 == 0:\n",
    "                print (i, all_losses2[-1], all_test_losses2[-1])\n",
    "            elif i < 500 and i % 100 == 0:\n",
    "                print (i, all_losses2[-1], all_test_losses2[-1])\n",
    "            if i % 500000 == 0:\n",
    "                torch.save([all_losses_alpha_more2, all_test_losses_alpha_more2], f'v8_alpha_experiment_run2_{alpha}_{j}.pt')\n",
    "        all_losses_alpha_more2[alpha] = all_losses2\n",
    "        all_test_losses_alpha_more2[alpha] = all_test_losses2\n",
    "        torch.save([all_losses_alpha_more2, all_test_losses_alpha_more2], f'v8_alpha_experiment_run2_{j}.pt')\n",
    "        all_res.append([all_losses_alpha_more2, all_test_losses_alpha_more2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [-2, -1, 0, 0.01, 0.05]\n",
    "trains = {}\n",
    "tests = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    trains[alpha] = []\n",
    "    tests[alpha] = []\n",
    "\n",
    "for j in range(10):\n",
    "    a1, a2 = torch.load(f'v8_alpha_experiment_run2_{j}.pt')\n",
    "    for alpha in alphas:\n",
    "        trains[alpha].append(a1[alpha][1000])\n",
    "        tests[alpha].append(a2[alpha][30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print (alpha, np.mean(trains[alpha]), np.mean(tests[alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2 0.0023977802265394794 0.06740795828737217\n",
      "-1 0.0015850432074508635 0.06514303480364823\n",
      "0 0.013449423925627958 0.17348444542169325\n",
      "0.01 0.011958419904337856 0.17462207729392137\n",
      "0.05 0.022404042458352952 0.21832848256192608\n"
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "    print (alpha, np.std(trains[alpha]), np.std(tests[alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2 = torch.load(f'v8_alpha_experiment_run2_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {-2:'o', -1:'s', 0:'^', 0.01:'<', 0.05:'>', -10:'v'}\n",
    "colors = {-1:'C1',-2:'C0',0:'C2', 0.05:'C4',0.01:'C3'}\n",
    "all_losses_alpha_more2 = a1\n",
    "all_test_losses_alpha_more2 = a2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# check_keys = sorted([j for j in all_losses_alpha_more2.keys()])\n",
    "check_keys = [-2, -1, 0, 0.01, 0.05]\n",
    "j = 0\n",
    "# markers = ['o', 's', '^', '<', '>', 'v', 'x', 'd', '*']\n",
    "for key in sorted(check_keys):\n",
    "    ax.plot(range(2, 1000), all_losses_alpha_more2[key][2:1000], f'{markers[key]}-', linewidth=3, \n",
    "            markersize=8, color=colors[key], markevery=240)\n",
    "    j += 1\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend([f'alpha = {j}' for j in check_keys], fontsize=24)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Number of Epochs\", fontsize=24)\n",
    "ax.set_ylabel(\"Training Error\", fontsize=24)\n",
    "# ax.set_ylim(0.9, 1.25e3)\n",
    "ax.set_xticklabels(['', \"0\", \"200\", \"400\", \"600\", \"800\", \"1,000\"], fontsize=24)\n",
    "ax.set_yticks([0.01, 0.1, 1, 10, 100, 1000])\n",
    "ax.set_ylim(0.009, 1500)\n",
    "ax.set_yticklabels(['$10^{-2}$',  '$10^{-1}$', '$10^{0}$', '$10^{1}$', '$10^{2}$', '$10^{3}$',], fontsize=24)\n",
    "ax.set_title(\"\")\n",
    "fig.tight_layout()\n",
    "fig.savefig('simulated_plot_train_v7', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {-2:'o', -1:'s', 0:'^', 0.01:'<', 0.05:'>', -10:'v'}\n",
    "colors = {-1:'C1',-2:'C0',0:'C2', 0.05:'C4',0.01:'C3'}\n",
    "all_losses_alpha_more2 = a1\n",
    "all_test_losses_alpha_more2 = a2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# check_keys = sorted([j for j in all_losses_alpha_more2.keys()])\n",
    "check_keys = [-2, -1, 0, 0.01, 0.05]\n",
    "j = 0\n",
    "for key in check_keys:\n",
    "    ax.plot(range(2, 102), all_test_losses_alpha_more2[key][2:102], f'{markers[key]}-', linewidth=3, \n",
    "            markersize=8, color=colors[key], markevery=24)\n",
    "    j += 1\n",
    "# ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend([f'alpha = {j}' for j in check_keys], fontsize=24)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Number of Epochs\", fontsize=24)\n",
    "ax.set_ylabel(\"Testing Error\", fontsize=24)\n",
    "ax.set_ylim(0.9, 1.25e3)\n",
    "ax.set_xticklabels(['', \"0\", \"20\", \"40\", \"60\", \"80\", '100'], fontsize=24)\n",
    "ax.set_yticklabels(['',  '',  '$10^{0}$', '$10^{1}$', '$10^{2}$', '$10^{3}$'], fontsize=24)\n",
    "ax.set_title(\"\")\n",
    "fig.tight_layout()\n",
    "fig.savefig('simulated_plot_test_v7', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, run the experiments with many choice of alpha and get theoretically predicted upper bounds of the convergence rates and numerical convergence rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# all_res = []\n",
    "all_losses_alpha_more2 = {}\n",
    "all_test_losses_alpha_more2 = {}\n",
    "m = 5000\n",
    "a = a.to(device)\n",
    "b = b.to(device)\n",
    "aa = aa.to(device)\n",
    "bb = bb.to(device)\n",
    "j = 0\n",
    "for j in range(10):\n",
    "    torch.manual_seed(j*77+2951)\n",
    "    for alpha in [-2.0, -1, 0.05, 0, 0.01, -3, -5, -7, -10, -0.1, -0.5, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "        z2 = nn.Sequential(nn.Linear(5, m), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, m), \n",
    "                          nn.LeakyReLU(alpha), \n",
    "                          Multiply(1./np.sqrt(1+alpha**2)), \n",
    "                          nn.Linear(m, 1))\n",
    "        nn.init.constant_(z2[0].bias, 0)\n",
    "        nn.init.normal_(z2[0].weight, mean=0, std=np.sqrt(1./m))\n",
    "        nn.init.constant_(z2[1].bias, 0)\n",
    "        nn.init.normal_(z2[1].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.normal_(z2[4].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[4].bias, 0)\n",
    "        nn.init.normal_(z2[7].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[7].bias, 0)\n",
    "        nn.init.normal_(z2[10].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[10].bias, 0)\n",
    "        nn.init.normal_(z2[13].weight, mean=0, std=np.sqrt(2./m))\n",
    "        nn.init.constant_(z2[13].bias, 0)\n",
    "        nn.init.normal_(z2[16].weight, mean=0, std=np.sqrt(1))\n",
    "        nn.init.constant_(z2[16].bias, 0)\n",
    "        z2 = z2.to(device)\n",
    "        all_losses2 = []\n",
    "        all_test_losses2 = []\n",
    "        op2 = optim.SGD(lr=1e-4, params=z2.parameters())\n",
    "        print (\"=============\", alpha, \"===============\")\n",
    "        for i in range(5000):\n",
    "            op2.zero_grad()\n",
    "            yh = z2(a).view(-1)\n",
    "            loss_use = loss(yh, b)\n",
    "            loss_use.backward()\n",
    "            op2.step()\n",
    "            all_losses2.append(loss_use.item())\n",
    "            yh = z2(aa).view(-1)\n",
    "            loss_use = loss(yh, bb)\n",
    "            all_test_losses2.append(loss_use.item())\n",
    "            if i % 5000 == 0:\n",
    "                print (i, all_losses2[-1], all_test_losses2[-1])\n",
    "            elif i < 500 and i % 100 == 0:\n",
    "                print (i, all_losses2[-1], all_test_losses2[-1])\n",
    "            if i % 500000 == 0:\n",
    "                torch.save([all_losses_alpha_more2, all_test_losses_alpha_more2], f'v8_alpha_experiment_run2_{alpha}_all_alphas_{j}.pt')\n",
    "        all_losses_alpha_more2[alpha] = all_losses2\n",
    "        all_test_losses_alpha_more2[alpha] = all_test_losses2\n",
    "        torch.save([all_losses_alpha_more2, all_test_losses_alpha_more2], f'v8_alpha_experiment_run2_all_alphas_{j}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_alphas = sorted([j for j in all_losses_alpha_more2.keys()])\n",
    "conv_rates = []\n",
    "alpha = -2\n",
    "for alpha in check_alphas:\n",
    "    res = (all_losses_alpha_more2[alpha][1000]/all_losses_alpha_more2[alpha][100])**(1./900)\n",
    "    conv_rates.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0\n",
    "const = 1 - (all_losses_alpha_more2[alpha][1000]/all_losses_alpha_more2[alpha][100])**(1./900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_thm_alphas = np.linspace(-10, 0.5, 100)\n",
    "pred_thm_convs = 1 - const*(1 - pred_thm_alphas)**2 / (1 + pred_thm_alphas**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(check_alphas, conv_rates, marker='o', linewidth = 4, markersize=8)\n",
    "ax.plot(pred_thm_alphas, pred_thm_convs, linewidth = 4, markersize=8)\n",
    "ax.legend(['Experimentally estimated\\n convergence rate', 'Theoretical upper bound\\n of convergence rate'], fontsize=24)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"$\\\\alpha$\", fontsize=24)\n",
    "ax.set_xticklabels(['', '-10', '-8', '-6', '-4', '-2', '0'], fontsize=18)\n",
    "ax.set_yticklabels(['', '0.9970', '0.9975', '0.9980', '0.9985', '0.9990', '0.9995'], fontsize=18)\n",
    "# ax.set_ylabel(\"Convergence Rate\", fontsize=24)\n",
    "fig.savefig('plot_for_all_alphas', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
